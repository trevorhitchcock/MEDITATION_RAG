{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd62eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19093f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>source_file</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cureus_cleaned.txt_chunk_0</td>\n",
       "      <td>cureus_cleaned.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>This article discusses the power of meditation...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cureus_cleaned.txt_chunk_1</td>\n",
       "      <td>cureus_cleaned.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>yield more beneficial clinical outcomes. Every...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cureus_cleaned.txt_chunk_2</td>\n",
       "      <td>cureus_cleaned.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>that constant reinforcement of happy thoughts ...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cureus_cleaned.txt_chunk_3</td>\n",
       "      <td>cureus_cleaned.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>energy and possibility is there remains a myst...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cureus_cleaned.txt_chunk_4</td>\n",
       "      <td>cureus_cleaned.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>benefits at the genetic or immunological level...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     chunk_id         source_file  chunk_index  \\\n",
       "0  cureus_cleaned.txt_chunk_0  cureus_cleaned.txt            0   \n",
       "1  cureus_cleaned.txt_chunk_1  cureus_cleaned.txt            1   \n",
       "2  cureus_cleaned.txt_chunk_2  cureus_cleaned.txt            2   \n",
       "3  cureus_cleaned.txt_chunk_3  cureus_cleaned.txt            3   \n",
       "4  cureus_cleaned.txt_chunk_4  cureus_cleaned.txt            4   \n",
       "\n",
       "                                                text  word_count  \n",
       "0  This article discusses the power of meditation...         150  \n",
       "1  yield more beneficial clinical outcomes. Every...         150  \n",
       "2  that constant reinforcement of happy thoughts ...         150  \n",
       "3  energy and possibility is there remains a myst...         150  \n",
       "4  benefits at the genetic or immunological level...         150  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df = pd.read_csv(\"../data/chunks/meditation_chunks.csv\")\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d298fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hitch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cf1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hitch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hitch\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\hitch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# location of ChromaDB\n",
    "INDEX_DIR = \"../data/index/meditations\"\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# Create a persistent Chroma client\n",
    "client = chromadb.PersistentClient(path=INDEX_DIR)\n",
    "\n",
    "# Create (or get) a collection\n",
    "collection = client.get_or_create_collection(name=\"meditations\")\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c893f606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunks 0–64\n",
      "Added chunks 64–128\n",
      "Added chunks 128–192\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "ids = chunks_df[\"chunk_id\"].tolist()\n",
    "documents = chunks_df[\"text\"].tolist()\n",
    "metadatas = chunks_df[[\"source_file\", \"chunk_index\", \"word_count\"]].to_dict(orient=\"records\")\n",
    "\n",
    "for start in range(0, len(documents), BATCH_SIZE):\n",
    "    end = start + BATCH_SIZE\n",
    "    batch_docs = documents[start:end]\n",
    "    batch_ids = ids[start:end]\n",
    "    batch_metadatas = metadatas[start:end]\n",
    "\n",
    "    # Embeddings for this batch\n",
    "    batch_embeddings = embedder.encode(batch_docs).tolist()\n",
    "\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        documents=batch_docs,\n",
    "        metadatas=batch_metadatas,\n",
    "        embeddings=batch_embeddings\n",
    "    )\n",
    "\n",
    "    print(f\"Added chunks {start}–{end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hitch\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:04<00:00, 18.1MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['your awareness. And as you observe thoughts, you may notice thoughts about the future or the past or thoughts about planning, evaluating, maybe fantasizing, or remembering. If you find it helpful, you may very gently and lightly label a thought silently to yourself with a single word such as \"past\", \"future\", or simply \"thinking\". Be aware of the thought, allowing it to be and pass, without having to identify with it or cling to it, just a thought being known. Labeling a thought may provide some distance, more objectivity, or spaciousness as you rest with an awareness of thoughts. As you observe thoughts, you may also notice emotions arising along with the thoughts. You may observe a variety of human emotions, including fear or peace, anxiety, contentment, anger, joy, or sadness. And you may also notice a variety of body sensations associated with these emotions, including perhaps a tightness, bracing,',\n",
       " \"fixated on the thought. So as much as you can, again, keeping the attention wide open; and if you get stuck using this very gentle whisper-like label or note, thinking, as a way to help untangle you from the thinking process. And remember that if the thinking goes away, the thoughts disappear, then you can relax into that quiet space of mind; a sense of spaciousness and openness. So as you continue to notice the thinking process being opened, any thoughts, allow yourself to be curious about if there are any physical components of thinking. Can you sense any pressure or tension in your body as you're thinking? Often, the face or the shoulders or the belly will hold some of this pressure or tension. And if you notice this, is it possible to relax and soften any of this tension or pressure that's connected to thinking? And this is\",\n",
       " 'of your awareness. As you observe thoughts, you may notice thoughts about the future or the past or thoughts about planning, evaluating, maybe fantasizing, or remembering. If you find it helpful, you may very gently and lightly label a thought silently to yourself with a single word such as \"past\", \"future\", or simply \"thinking\". Be aware of the thought, allowing it to be and pass, without having to identify with it or cling to it, just a thought being known. Labeling a thought may provide some distance, more objectivity, or spaciousness as you rest with an awareness of thoughts. As you observe thoughts, you may also notice emotions arising along with the thoughts. You may observe a variety of human emotions, including fear or peace, anxiety, contentment, anger, joy, or sadness. And you may also notice a variety of body sensations associated with these emotions, including perhaps a tightness, bracing,']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test query\n",
    "query = \"grounding myself when I feel anxious\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "results[\"documents\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810872db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
