{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8e0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e3b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hitch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Re-open the existing Chroma index\n",
    "INDEX_DIR = \"../data/index/meditations\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=INDEX_DIR)\n",
    "collection = client.get_collection(name=\"meditations\")\n",
    "\n",
    "# Same embedding model you used when building the index\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2087fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c8edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG Helper Functions ---\n",
    "\n",
    "def retrieve_context(query: str, collection, embedder, k: int = 4):\n",
    "    \"\"\"Retrieve top-k most relevant chunks using Chroma.\"\"\"\n",
    "    query_emb = embedder.encode([query]).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=k\n",
    "    )\n",
    "    docs = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    return docs, metadatas\n",
    "\n",
    "\n",
    "def build_prompt(query: str, retrieved_docs: list[str]) -> str:\n",
    "    \"\"\"Build a clean, meditation-friendly prompt using retrieved context.\"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
    "    prompt = f\"\"\"\n",
    "Using the reference texts below, write a calm, secular guided meditation that answers the userâ€™s request.\n",
    "\n",
    "Reference texts:\n",
    "{context}\n",
    "\n",
    "User request:\n",
    "{query}\n",
    "\n",
    "Now write a clear, compassionate response. \n",
    "Speak directly to the listener in second person (\"you\").\n",
    "Do NOT mention the reference texts or describe your process.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "def generate_with_rag(\n",
    "    query: str,\n",
    "    collection,\n",
    "    embedder,\n",
    "    llm,\n",
    "    k: int = 4,\n",
    "    max_new_tokens: int = 300\n",
    "):\n",
    "    \"\"\"Retrieve relevant text, build a prompt, and generate an LLM response.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve\n",
    "    docs, metas = retrieve_context(query, collection, embedder, k=k)\n",
    "\n",
    "    # 2. Build prompt\n",
    "    prompt = build_prompt(query, docs)\n",
    "\n",
    "    # 3. Generate\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    return output, docs, metas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8f79c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick, quiet meditation. :]\n"
     ]
    }
   ],
   "source": [
    "query = \"Create a short grounding meditation for anxiety before bed.\"\n",
    "response, docs_used, metas_used = generate_with_rag(query, collection, embedder, llm)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b07b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
